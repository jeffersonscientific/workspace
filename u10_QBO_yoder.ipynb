{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCL stuff:\n",
    "#import Ngl, Nio\n",
    "import Nio\n",
    "#import pyncl\n",
    "#\n",
    "import pylab as plt\n",
    "import numpy\n",
    "#import scipy\n",
    "#\n",
    "import os,sys\n",
    "#import pathlib\n",
    "\n",
    "import getpass\n",
    "import subprocess\n",
    "import contextlib\n",
    "\n",
    "def file_report(fin):\n",
    "    dims = fin.dimensions.values()\n",
    "    dimnames = fin.dimensions.keys()\n",
    "    #\n",
    "    varnames_all = fin.variables.keys()\n",
    "    varnames = [s for s in varnames_all if not s in dimnames]\n",
    "    #\n",
    "    print('** dims: ', list(zip(dimnames, dims)))\n",
    "    #\n",
    "    print('** varnames_all: ', varnames_all)\n",
    "    print('** varnames: ', varnames)\n",
    "    #\n",
    "\n",
    "    print('type(fin.dimensions): ', type(fin.dimensions) )\n",
    "    #\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount network resource (sshfs), if we are working locally\n",
    "- This only if we want to pull down a (small) subset of data to our local machine\n",
    "- use subprocess() to mount the remote FS\n",
    "- Note we do * **not** * write our password in code, pull it from a (n unencrypted) data file, etc. In fact, in this script, we don't even write it to a variable\n",
    "- If working on a tool server, etc. this drive will (probable) already be mounted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** fmount: /home/myoder96/mazama_data\n"
     ]
    }
   ],
   "source": [
    "#do_mount = True\n",
    "do_mount = False\n",
    "#\n",
    "f_mount = os.path.join(os.environ['HOME'], 'mazama_data')\n",
    "print('** fmount: {}'.format(f_mount))\n",
    "#\n",
    "mazama_umount_data = 'umount {}'.format(f_mount)\n",
    "#\n",
    "if do_mount:\n",
    "    # this should work, in principle, but it is not (so far), and maybe for good reasons. I've definitely\n",
    "    #. read taht using the -o password_stdin option sometimes does not work.\n",
    "    #ssh_password = getpass.getpass('ssh password: ')\n",
    "    #\n",
    "    # sshfs cees-tool-7:/data ~/mazama_data -o password_stdin -o volname=mazama_data <<<\n",
    "    #\n",
    "    # set up sshfs call here...source\n",
    "    mazama_mount_data = 'sshfs cees-tool-7:/data {} -o password_stdin -o volname=mazama_data <<< {}'\n",
    "    #\n",
    "    #\n",
    "    #subprocess.call(mkdir_command, shell=True)\n",
    "    subprocess.call(mazama_mount_data.format(f_mount, getpass.getpass()), shell=True)\n",
    "    #subprocess.call(unmount_command, shell=True)\n",
    "    #\n",
    "    #del ssh_password\n",
    "    #source\n",
    "    print('ls f_mount: \\n', os.listdir(f_mount))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** pth_name: /data/ESS/regirock/cesm_archive/Rachel/U_V_T_Z3_plWACCMSC_CTL_122.cam.h2.0001-0202.nc\n"
     ]
    }
   ],
   "source": [
    "# NOTE: probably need to connect (sshfs) to NFS (sshfs cees-tool-7:/data {f_mount})\n",
    "input_file_path = 'ESS/regirock/cesm_archive/Rachel/U_V_T_Z3_plWACCMSC_CTL_122.cam.h2.0001-0202.nc'\n",
    "f_mount='/data'\n",
    "input_file_path = os.path.join(f_mount, input_file_path)\n",
    "#\n",
    "#output_file_path = 'my_output.nc'\n",
    "output_file_path = os.path.join(os.environ['HOME'], 'Codes/temp/my_output.nc')\n",
    "#\n",
    "#input_file_path = os.path.join(os.environ['HOME'], input_file_path)\n",
    "\n",
    "#\n",
    "print('** pth_name: {}'.format(input_file_path))\n",
    "\n",
    "#\n",
    "#\n",
    "# # we can also use pathlib to construct the path, but it does not really gain us much.\n",
    "# #  nominally, we should construct the path from an orderd list of parts ['mazama_data', 'ESS', ...]\n",
    "# #. and then .join() them, in the event that sommebody tries this from Windows. But for now,\n",
    "# #. we'll let Windows usiers fix it themselves (if need be)...\n",
    "#\n",
    "# TODO: reorganize using a context manager. Nio.open_file() will not take a CM directly, but we \n",
    "#. can make one using contextlib:\n",
    "# with contextlib.closing(Nio.open_file(input_file_path, 'r')) as fin:\n",
    "#.  etc...\n",
    "#.  etc...\n",
    "# open the file; use a context handler:\n",
    "# ... or maybe not. seems to not be compatible...\n",
    "#\n",
    "fin = Nio.open_file(input_file_path, 'r')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data properties\n",
    "- We've opend a data file handle...\n",
    "- Now, let's look at some properties of the data set:\n",
    "    - dimension and dimension names\n",
    "    - Note that rank is the size of the dimensions array (dimensions of dimensions)\n",
    "    - Note also the distinction between dimensions and variables. \n",
    "    - From a strict data-modeling standpoint, this is sort of silly, but it makes sense that it is the minimum (maximum?) level of slicing. After the dimensions, are attributes that group together... sort of.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('** dims: ', [('lat', 96), ('lon', 144), ('lev_p', 13), ('time', 73730)])\n",
      "('** varnames_all: ', ['PV', 'V', 'lon', 'U', 'lev_p', 'time', 'lat', 'Z3', 'T'])\n",
      "('** varnames: ', ['PV', 'V', 'U', 'Z3', 'T'])\n",
      "('type(fin.dimensions): ', <type 'dict'>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "file_report(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lats: ', [-90.0, -88.10526315789474, -86.21052631578948, -84.3157894736842, -82.42105263157895, -80.52631578947368, -78.63157894736842, -76.73684210526316, -74.84210526315789, -72.94736842105263, -71.05263157894737, -69.15789473684211, -67.26315789473685, -65.36842105263158, -63.473684210526315, -61.578947368421055, -59.684210526315795, -57.78947368421053, -55.89473684210527, -54.0, -52.10526315789474, -50.21052631578947, -48.31578947368421, -46.42105263157895, -44.526315789473685, -42.631578947368425, -40.73684210526316, -38.8421052631579, -36.94736842105264, -35.05263157894737, -33.15789473684211, -31.263157894736842, -29.368421052631582, -27.473684210526322, -25.578947368421055, -23.684210526315795, -21.789473684210535, -19.89473684210526, -18.0, -16.10526315789474, -14.21052631578948, -12.31578947368422, -10.421052631578945, -8.526315789473685, -6.631578947368425, -4.736842105263165, -2.8421052631579045, -0.9473684210526301, 0.9473684210526301, 2.8421052631578902, 4.73684210526315, 6.631578947368411, 8.526315789473685, 10.421052631578945, 12.315789473684205, 14.210526315789465, 16.105263157894726, 18.0, 19.89473684210526, 21.78947368421052, 23.68421052631578, 25.57894736842104, 27.473684210526315, 29.368421052631575, 31.263157894736835, 33.157894736842096, 35.052631578947356, 36.94736842105263, 38.84210526315789, 40.73684210526315, 42.63157894736841, 44.52631578947367, 46.42105263157893, 48.31578947368419, 50.21052631578948, 52.10526315789474, 54.0, 55.89473684210526, 57.78947368421052, 59.68421052631578, 61.57894736842104, 63.4736842105263, 65.36842105263156, 67.26315789473682, 69.15789473684211, 71.05263157894737, 72.94736842105263, 74.84210526315789, 76.73684210526315, 78.63157894736841, 80.52631578947367, 82.42105263157893, 84.31578947368419, 86.21052631578945, 88.10526315789474, 90.0])\n",
      "('lons: ', [0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0, 22.5, 25.0, 27.5, 30.0, 32.5, 35.0, 37.5, 40.0, 42.5, 45.0, 47.5, 50.0, 52.5, 55.0, 57.5, 60.0, 62.5, 65.0, 67.5, 70.0, 72.5, 75.0, 77.5, 80.0, 82.5, 85.0, 87.5, 90.0, 92.5, 95.0, 97.5, 100.0, 102.5, 105.0, 107.5, 110.0, 112.5, 115.0, 117.5, 120.0, 122.5, 125.0, 127.5, 130.0, 132.5, 135.0, 137.5, 140.0, 142.5, 145.0, 147.5, 150.0, 152.5, 155.0, 157.5, 160.0, 162.5, 165.0, 167.5, 170.0, 172.5, 175.0, 177.5, 180.0, 182.5, 185.0, 187.5, 190.0, 192.5, 195.0, 197.5, 200.0, 202.5, 205.0, 207.5, 210.0, 212.5, 215.0, 217.5, 220.0, 222.5, 225.0, 227.5, 230.0, 232.5, 235.0, 237.5, 240.0, 242.5, 245.0, 247.5, 250.0, 252.5, 255.0, 257.5, 260.0, 262.5, 265.0, 267.5, 270.0, 272.5, 275.0, 277.5, 280.0, 282.5, 285.0, 287.5, 290.0, 292.5, 295.0, 297.5, 300.0, 302.5, 305.0, 307.5, 310.0, 312.5, 315.0, 317.5, 320.0, 322.5, 325.0, 327.5, 330.0, 332.5, 335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5])\n",
      "('p10: ', 10)\n",
      "('len(time): ', 73730)\n"
     ]
    }
   ],
   "source": [
    "# note that the lat and lon variables are small (they are basically just indices):\n",
    "lats = fin.variables['lat']\n",
    "lons = fin.variables['lon']\n",
    "#p_levs = fin.variables['lev_p']\n",
    "p10 = fin.variables['lev_p'][2]\n",
    "#\n",
    "print('lats: ', [y for y in lats])\n",
    "print('lons: ', [x for x in lons])\n",
    "print('p10: ', p10)\n",
    "tm   = fin.variables['time']\n",
    "print('len(time): ', len(tm))\n",
    "#print('p_levs: ', [p for p in p_levs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the raw data\n",
    "- Open the raw/source data file (already done!)\n",
    "- Take some 'slice' of our variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udum: \n",
      "100\n",
      "('** ', (100, 96, 144))\n",
      "udum2: \n",
      "100\n",
      "('** ', (100, 96, 144))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: set up batching, and see if we can use a += syntax when we write to the output file.\n",
    "batch_size = 100\n",
    "udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "# we can get a second (subsequent) slice like this:\n",
    "udum2 = fin.variables['U'][len(udum):len(udum)+batch_size, 2,:,:]\n",
    "#\n",
    "print('udum: ')\n",
    "print(len(udum))\n",
    "print('** ', udum.shape)\n",
    "#\n",
    "#\n",
    "print('udum2: ')\n",
    "print(len(udum2))\n",
    "print('** ', udum2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a subset of the data or a derived data set:\n",
    "- Create a file for export\n",
    "- Note that there are good and bad ways to do this (usually trading code complexity for speed)\n",
    "- For optimal speed, we pre-define as much of the output as possible\n",
    "- For simplicity, we might leave our principal dimension undefined -- or something like that.\n",
    "- Note: we can open our file in modes; {'c': create,'r': read, {'w', 'r+', 'a', ???}: read+, append, write} \n",
    "\n",
    "#### What We are Going to Do:\n",
    "- Create the output file\n",
    "- Discuss in comments some file-writng methods, strategies, etc.\n",
    "- Then do the file writing (exporting) in the following pane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 3 must be tuple, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-844989004314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# set the dimension variable values as well:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/myoder96/.conda/envs/ncl_stable/lib/python2.7/site-packages/PyNIO/Nio.pyc\u001b[0m in \u001b[0;36mcreate_variable\u001b[0;34m(self, name, type, dimensions)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;31m#print 'in create variable'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         vp  = _proxy(v,'str','len',\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 3 must be tuple, not str"
     ]
    }
   ],
   "source": [
    "# create the file. define everything!\n",
    "#\n",
    "#\n",
    "# time dimension size:\n",
    "# we can get the full dimension size, if we are going to write the whole lot;\n",
    "# otherwise, maybe just a subset?\n",
    "#n_time = fin.dimensions['time']\n",
    "# assume we'll take 10 iterations (or so)\n",
    "n_batches = 10\n",
    "n_time = n_batches*batch_size\n",
    "#\n",
    "os.system('rm {}'.format(output_file_path))\n",
    "# do we need to delete the file?\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'c')) as fout:\n",
    "    #fout = Nio.open_file(output_file_path, 'c')\n",
    "    fout.create_dimension('time', n_time)\n",
    "    fout.create_dimension('lat', fin.dimensions['lat'])\n",
    "    fout.create_dimension('lon', fin.dimensions['lon'])\n",
    "    #\n",
    "    # set the dimension variable values as well:\n",
    "    for v_name in ('lat', 'lon', 'time'):\n",
    "        fout.create_variable(v_name, 'f', v_name)\n",
    "        fout.variables[v_name] = fin.variables['v_name']\n",
    "    #\n",
    "    fout.create_variable('U','f',('time','lat','lon'))\n",
    "    setattr(fout.variables['U'], 'standard_name', 'pressure')\n",
    "    setattr(fout.variables['U'], 'units', 'kPa')\n",
    "    #\n",
    "    # The top syntax is required for scaler, non-indexed values (or so I have read).\n",
    "    #. we can also use it for arrays, but only if we are assigning the whole value -- in other words, the \n",
    "    #. dimensinos must match. (this works if dim(udum)==dim(fout)). I assume the exception is if the output\n",
    "    #. dimension is undefined.\n",
    "    #\n",
    "    #fout.variables['u'].assign_value(udum)        # this works if dimensions align\n",
    "    #fout.variables['u'][0:len(udum)] = udum      # use this for partial assignment (note, i think this does allow \n",
    "    #                                            # (aka, will expand) for overflow -- [k,j] > len(fout.variable) )\n",
    "    #\n",
    "    # if this is going to take a long time, it might be smart to close and explicitly bufer.\n",
    "\n",
    "    #\n",
    "    # and we might want to close the file here:\n",
    "    print('file created: ')\n",
    "    print(file_report(fout))\n",
    "    #fout.close()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our work. What have we written?\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'r')) as fin_check:\n",
    "    print('Some File Info: {}'.format(output_file_path))\n",
    "    print(file_report(fin_check))\n",
    "    #\n",
    "    fin_check.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to the file\n",
    "- Our basic operation is:\n",
    "    - Read a batch of data from the source file (fin)\n",
    "    - Do whatever we need to do to said data\n",
    "    - Write something (in this case, just the data) to the output file\n",
    "- We have soeme choices:\n",
    "    - Buffer to a memory variable (aka, read into a local memory variable, then write that variable to output)\n",
    "    - Internal buffers only (fout.variables[my_var_out][k:j] = fin.variables[my_var_in][l:m] )\n",
    "        - For simple data transfers, this is probaby the best approach. In fact, we can probably skip batching, since the NetCDF objects are probably smart enough to manage memory.\n",
    "    - Parallelization: \n",
    "        - Depending on where latency occurs, etc., there may be opportunity to speed this up via reading and writing in parallel channels. Basically, run separate threads over a set of index ranges.\n",
    "        - Also, for simple filering or processing, the data transfer may be slow enough that it would make sense to parallelize the read/process steps (aka, extract a batch; submit to a processing thread, extract the next bit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, demonstrate read/write with batches. It will be important to understand how to do this if we ever want\n",
    "#. to do something more complicated that a straight transfer or simple filter.\n",
    "# note: we don't need the same batch sizes:\n",
    "batch_size_write = 200\n",
    "n_batches = int(numpy.ceil(n_time)/batch_size_write)\n",
    "#\n",
    "print('*** writing {} batches, of size {} (last batch might be smaller)'.format(n_batches, batch_size_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# fib should already be open:\n",
    "#fin = Nio.open_file(input_file_path, 'r')\n",
    "#\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'a')) as fout:\n",
    "    # 'a', 'w', 'r+' all mean (read, write/append)\n",
    "    #\n",
    "    # extract data like: udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "    for j,k in enumerate(range(0, n_time, batch_size_write)):\n",
    "        #print('** ** [{}:{}]'.format(k, k+batch_size_write))\n",
    "        #\n",
    "        # remember, our read-data are like:\n",
    "        # udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "        # and we're making out output file more or less a mirror of that:\n",
    "        # note: we never explicilty store the data in a local variable; the memory footprint\n",
    "        #. is determined entirely by the NetCDF class, so really we can probably do this in one batch...\n",
    "        #  unless we want to parallelize, in which case we can run each of thsed batches an a Process() thread,\n",
    "        # in a Queue(), Pool(), etc.\n",
    "        print('begin k={}/{} batches.'.format(j, n_batches))\n",
    "        fout.variables['U'][k:k+batch_size_write] = fin.variables['U'][k:k+batch_size_write, 2,:,:]\n",
    "#\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.closing(Nio.open_file(output_file_path, 'r')) as f:\n",
    "    print('shape subset file: ', f.variables['U'].shape)\n",
    "#\n",
    "with contextlib.closing(Nio.open_file(input_file_path, 'r')) as fin:\n",
    "    #print('shape input file: ', fin.variables['U'].shape)\n",
    "    print('shape our slice: ', fin.variables['U'][:,2,:,:].shape)\n",
    "    print('shape input file: ', fin.variables['U'].shape)\n",
    "    #\n",
    "    file_report(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ncl_stable]",
   "language": "python",
   "name": "conda-env-ncl_stable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
