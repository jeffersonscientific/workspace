{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCL stuff:\n",
    "import Ngl, Nio\n",
    "#import pyncl\n",
    "#\n",
    "import pylab as plt\n",
    "import numpy, scipy\n",
    "#\n",
    "import os,sys\n",
    "import pathlib\n",
    "\n",
    "import getpass\n",
    "import subprocess\n",
    "import contextlib\n",
    "\n",
    "def file_report(fin):\n",
    "    dims = fin.dimensions.values()\n",
    "    dimnames = fin.dimensions.keys()\n",
    "    #\n",
    "    varnames_all = fin.variables.keys()\n",
    "    varnames = [s for s in varnames_all if not s in dimnames]\n",
    "    #\n",
    "    print('** dims: ', list(zip(dimnames, dims)))\n",
    "    #\n",
    "    print('** varnames_all: ', varnames_all)\n",
    "    print('** varnames: ', varnames)\n",
    "    #\n",
    "\n",
    "    print('type(fin.dimensions): ', type(fin.dimensions) )\n",
    "    #\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount network resource (sshfs), if we are working locally\n",
    "- This only if we want to pull down a (small) subset of data to our local machine\n",
    "- use subprocess() to mount the remote FS\n",
    "- Note we do * **not** * write our password in code, pull it from a (n unencrypted) data file, etc. In fact, in this script, we don't even write it to a variable\n",
    "- If working on a tool server, etc. this drive will (probable) already be mounted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do_mount = True\n",
    "do_mount = False\n",
    "#\n",
    "f_mount = os.path.join(os.environ['HOME'], 'mazama_data')\n",
    "print('** fmount: {}'.format(f_mount))\n",
    "#\n",
    "mazama_umount_data = 'umount {}'.format(f_mount)\n",
    "#\n",
    "if do_mount:\n",
    "    # this should work, in principle, but it is not (so far), and maybe for good reasons. I've definitely\n",
    "    #. read taht using the -o password_stdin option sometimes does not work.\n",
    "    #ssh_password = getpass.getpass('ssh password: ')\n",
    "    #\n",
    "    # sshfs cees-tool-7:/data ~/mazama_data -o password_stdin -o volname=mazama_data <<<\n",
    "    #\n",
    "    # set up sshfs call here...source\n",
    "    mazama_mount_data = 'sshfs cees-tool-7:/data {} -o password_stdin -o volname=mazama_data <<< {}'\n",
    "    #\n",
    "    #\n",
    "    #subprocess.call(mkdir_command, shell=True)\n",
    "    subprocess.call(mazama_mount_data.format(f_mount, getpass.getpass()), shell=True)\n",
    "    #subprocess.call(unmount_command, shell=True)\n",
    "    #\n",
    "    #del ssh_password\n",
    "    #source\n",
    "    print('ls f_mount: \\n', os.listdir(f_mount))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: probably need to connect (sshfs) to NFS (sshfs cees-tool-7:/data {f_mount})\n",
    "input_file_path = 'ESS/regirock/cesm_archive/Rachel/U_V_T_Z3_plWACCMSC_CTL_122.cam.h2.0001-0202.nc'\n",
    "output_file_path = 'my_output.nc'\n",
    "#\n",
    "#f_mount = 'tmp'\n",
    "input_file_path = os.path.join(f_mount, input_file_path)\n",
    "#input_file_path = os.path.join(os.environ['HOME'], input_file_path)\n",
    "\n",
    "#\n",
    "print('** pth_name: {}'.format(input_file_path))\n",
    "\n",
    "#\n",
    "#\n",
    "# # we can also use pathlib to construct the path, but it does not really gain us much.\n",
    "# #  nominally, we should construct the path from an orderd list of parts ['mazama_data', 'ESS', ...]\n",
    "# #. and then .join() them, in the event that sommebody tries this from Windows. But for now,\n",
    "# #. we'll let Windows usiers fix it themselves (if need be)...\n",
    "#\n",
    "# TODO: reorganize using a context manager. Nio.open_file() will not take a CM directly, but we \n",
    "#. can make one using contextlib:\n",
    "# with contextlib.closing(Nio.open_file(input_file_path, 'r')) as fin:\n",
    "#.  etc...\n",
    "#.  etc...\n",
    "# open the file; use a context handler:\n",
    "# ... or maybe not. seems to not be compatible...\n",
    "#\n",
    "fin = Nio.open_file(input_file_path, 'r')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data properties\n",
    "- We've opend a data file handle...\n",
    "- Now, let's look at some properties of the data set:\n",
    "    - dimension and dimension names\n",
    "    - Note that rank is the size of the dimensions array (dimensions of dimensions)\n",
    "    - Note also the distinction between dimensions and variables. \n",
    "    - From a strict data-modeling standpoint, this is sort of silly, but it makes sense that it is the minimum (maximum?) level of slicing. After the dimensions, are attributes that group together... sort of.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "file_report(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the lat and lon variables are small (they are basically just indices):\n",
    "lats = fin.variables['lat']\n",
    "lons = fin.variables['lon']\n",
    "#p_levs = fin.variables['lev_p']\n",
    "p10 = fin.variables['lev_p'][2]\n",
    "#\n",
    "print('lats: ', [y for y in lats])\n",
    "print('lons: ', [x for x in lons])\n",
    "print('p10: ', p10)\n",
    "tm   = fin.variables['time']\n",
    "print('len(time): ', len(tm))\n",
    "#print('p_levs: ', [p for p in p_levs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the raw data\n",
    "- Open the raw/source data file (already done!)\n",
    "- Take some 'slice' of our variable of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: set up batching, and see if we can use a += syntax when we write to the output file.\n",
    "batch_size = 100\n",
    "udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "# we can get a second (subsequent) slice like this:\n",
    "udum2 = fin.variables['U'][len(udum):len(udum)+batch_size, 2,:,:]\n",
    "#\n",
    "print('udum: ')\n",
    "print(len(udum))\n",
    "print('** ', udum.shape)\n",
    "#\n",
    "#\n",
    "print('udum2: ')\n",
    "print(len(udum2))\n",
    "print('** ', udum2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a subset of the data or a derived data set:\n",
    "- Create a file for export\n",
    "- Note that there are good and bad ways to do this (usually trading code complexity for speed)\n",
    "- For optimal speed, we pre-define as much of the output as possible\n",
    "- For simplicity, we might leave our principal dimension undefined -- or something like that.\n",
    "- Note: we can open our file in modes; {'c': create,'r': read, {'w', 'r+', 'a', ???}: read+, append, write} \n",
    "\n",
    "#### What We are Going to Do:\n",
    "- Create the output file\n",
    "- Discuss in comments some file-writng methods, strategies, etc.\n",
    "- Then do the file writing (exporting) in the following pane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file. define everything!\n",
    "#\n",
    "#\n",
    "# time dimension size:\n",
    "# we can get the full dimension size, if we are going to write the whole lot;\n",
    "# otherwise, maybe just a subset?\n",
    "#n_time = fin.dimensions['time']\n",
    "# assume we'll take 10 iterations (or so)\n",
    "n_batches = 10\n",
    "n_time = n_batches*batch_size\n",
    "#\n",
    "os.system('rm {}'.format(output_file_path))\n",
    "# do we need to delete the file?\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'c')) as fout:\n",
    "    #fout = Nio.open_file(output_file_path, 'c')\n",
    "    fout.create_dimension('time', n_time)\n",
    "    fout.create_dimension('lat', fin.dimensions['lat'])\n",
    "    fout.create_dimension('lon', fin.dimensions['lon'])\n",
    "    #\n",
    "    # set the dimension variable values as well:\n",
    "    for v_name in ('lat', 'lon', 'time'):\n",
    "        fout.create_variable(v_name, 'f', v_name)\n",
    "        fout.variables[v_name] = fin.variables['v_name']\n",
    "    #\n",
    "    fout.create_variable('U','f',('time','lat','lon'))\n",
    "    setattr(fout.variables['U'], 'standard_name', 'pressure')\n",
    "    setattr(fout.variables['U'], 'units', 'kPa')\n",
    "    #\n",
    "    # The top syntax is required for scaler, non-indexed values (or so I have read).\n",
    "    #. we can also use it for arrays, but only if we are assigning the whole value -- in other words, the \n",
    "    #. dimensinos must match. (this works if dim(udum)==dim(fout)). I assume the exception is if the output\n",
    "    #. dimension is undefined.\n",
    "    #\n",
    "    #fout.variables['u'].assign_value(udum)        # this works if dimensions align\n",
    "    #fout.variables['u'][0:len(udum)] = udum      # use this for partial assignment (note, i think this does allow \n",
    "    #                                            # (aka, will expand) for overflow -- [k,j] > len(fout.variable) )\n",
    "    #\n",
    "    # if this is going to take a long time, it might be smart to close and explicitly bufer.\n",
    "\n",
    "    #\n",
    "    # and we might want to close the file here:\n",
    "    print('file created: ')\n",
    "    print(file_report(fout))\n",
    "    #fout.close()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our work. What have we written?\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'r')) as fin_check:\n",
    "    print('Some File Info: {}'.format(output_file_path))\n",
    "    print(file_report(fin_check))\n",
    "    #\n",
    "    fin_check.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to the file\n",
    "- Our basic operation is:\n",
    "    - Read a batch of data from the source file (fin)\n",
    "    - Do whatever we need to do to said data\n",
    "    - Write something (in this case, just the data) to the output file\n",
    "- We have soeme choices:\n",
    "    - Buffer to a memory variable (aka, read into a local memory variable, then write that variable to output)\n",
    "    - Internal buffers only (fout.variables[my_var_out][k:j] = fin.variables[my_var_in][l:m] )\n",
    "        - For simple data transfers, this is probaby the best approach. In fact, we can probably skip batching, since the NetCDF objects are probably smart enough to manage memory.\n",
    "    - Parallelization: \n",
    "        - Depending on where latency occurs, etc., there may be opportunity to speed this up via reading and writing in parallel channels. Basically, run separate threads over a set of index ranges.\n",
    "        - Also, for simple filering or processing, the data transfer may be slow enough that it would make sense to parallelize the read/process steps (aka, extract a batch; submit to a processing thread, extract the next bit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, demonstrate read/write with batches. It will be important to understand how to do this if we ever want\n",
    "#. to do something more complicated that a straight transfer or simple filter.\n",
    "# note: we don't need the same batch sizes:\n",
    "batch_size_write = 200\n",
    "n_batches = int(numpy.ceil(n_time)/batch_size_write)\n",
    "#\n",
    "print('*** writing {} batches, of size {} (last batch might be smaller)'.format(n_batches, batch_size_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# fib should already be open:\n",
    "#fin = Nio.open_file(input_file_path, 'r')\n",
    "#\n",
    "with contextlib.closing(Nio.open_file(output_file_path, 'a')) as fout:\n",
    "    # 'a', 'w', 'r+' all mean (read, write/append)\n",
    "    #\n",
    "    # extract data like: udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "    for j,k in enumerate(range(0, n_time, batch_size_write)):\n",
    "        #print('** ** [{}:{}]'.format(k, k+batch_size_write))\n",
    "        #\n",
    "        # remember, our read-data are like:\n",
    "        # udum = fin.variables['U'][0:batch_size, 2,:,:]\n",
    "        # and we're making out output file more or less a mirror of that:\n",
    "        # note: we never explicilty store the data in a local variable; the memory footprint\n",
    "        #. is determined entirely by the NetCDF class, so really we can probably do this in one batch...\n",
    "        #  unless we want to parallelize, in which case we can run each of thsed batches an a Process() thread,\n",
    "        # in a Queue(), Pool(), etc.\n",
    "        print('begin k={}/{} batches.'.format(j, n_batches))\n",
    "        fout.variables['U'][k:k+batch_size_write] = fin.variables['U'][k:k+batch_size_write, 2,:,:]\n",
    "#\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.closing(Nio.open_file(output_file_path, 'r')) as f:\n",
    "    print('shape subset file: ', f.variables['U'].shape)\n",
    "#\n",
    "with contextlib.closing(Nio.open_file(input_file_path, 'r')) as fin:\n",
    "    #print('shape input file: ', fin.variables['U'].shape)\n",
    "    print('shape our slice: ', fin.variables['U'][:,2,:,:].shape)\n",
    "    print('shape input file: ', fin.variables['U'].shape)\n",
    "    #\n",
    "    file_report(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ncl_stable]",
   "language": "python",
   "name": "conda-env-ncl_stable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
